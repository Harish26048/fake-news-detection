# -*- coding: utf-8 -*-
""""Think Before You Share: Say No to Fake News!"

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/harishthangavel/think-before-you-share-say-no-to-fake-news.131fccb2-38b9-4bd4-97d7-045b0072c0fa.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250910/auto/storage/goog4_request%26X-Goog-Date%3D20250910T134315Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D5e981386e62325ddd0bb1fd639dfc641a57a599f8358cd8c8fa5fbb157fcff3202dbf5808925bee312da8c2c5e5ec519c36a2f04ff7a6e5ccf9f13d033b585fefc9ada365ae8116f4953da9dc8965025978f4e1f1e7284c4e1e7c1e15146b180fe9000a506faed5ce3e454f274b291f80c99b17ee42758a065fee8ccc3e37cb9a1b862133b001c6b9f0c2866819441af9b67d4e5394c65b599ec2a3de5b97567b4436ba118443f8402b06b27e86540de44864f5c246ac35e27329fde352a07d2cea5c97b7d05073425b8f434171cd13ce7691b7287b455204ed33300b5ba1ccb0f7b05b3b86dfaad167f95698c000f6fc11395a4224f77f4000c73ffe2fa305b
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.
import kagglehub
clmentbisaillon_fake_and_real_news_dataset_path = kagglehub.dataset_download('clmentbisaillon/fake-and-real-news-dataset')

print('Data source import complete.')

"""**Executive Summary: Fake News Detection Analysis**

This analysis aimed to build and evaluate machine learning models to classify news articles as either "true" or "fake" based on their text content.

**Key Steps:**

1.  **Data Loading and Preparation:** Two datasets, 'True.csv' and 'Fake.csv', were loaded and combined into a single DataFrame. A 'label' column was added to distinguish between true and fake news. Duplicate rows were removed, and missing values were checked (none found).
2.  **Text Cleaning and Tokenization:** The 'text' and 'title' columns underwent cleaning, including converting text to lowercase, removing punctuation, and removing common English stopwords. The cleaned text was then tokenized into individual words.
3.  **Feature Extraction:** The cleaned and tokenized text and title were combined, and TF-IDF (Term Frequency-Inverse Document Frequency) was used to convert the text data into numerical features, creating a vector representation of the text.
4.  **Model Selection and Training:** Four classification models were selected: Multinomial Naive Bayes, Logistic Regression, Linear SVC, and Random Forest Classifier. The data was split into training and testing sets (80% train, 20% test), and each model was trained on the training data.
5.  **Model Evaluation:** The trained models were evaluated on the test set using several metrics: accuracy, precision, recall, and F1-score. The Linear SVC model demonstrated the highest F1-score, indicating the best balance between precision and recall for this task.
6.  **Hyperparameter Tuning (Linear SVC):** The Linear SVC model, identified as the best performer, was further tuned using GridSearchCV to find the optimal hyperparameters ('C', 'loss', and 'max\_iter'). The tuning process resulted in slightly different performance metrics for the tuned model compared to the initial evaluation, with a minor decrease in accuracy, precision, and F1-score.
7.  **Feature Importance Analysis:** The coefficients from the best-tuned Linear SVC model were analyzed to identify the most important features (words) that contributed to the classification. The top 20 features based on absolute coefficient values were identified and visualized. Words with positive coefficients are more associated with 'true' news, while words with negative coefficients are more associated with 'fake' news.

**Conclusion:**

The Linear SVC model, particularly after hyperparameter tuning, proved to be the most effective model for this fake news detection task based on the evaluation metrics. The feature importance analysis provides insights into the words that are most indicative of true or fake news, which can be valuable for understanding the characteristics of each type of news and potentially for advising the public on how to identify potentially fake news.

# cleaning n tokenization

Import all datasets that will be analyzed
"""

import pandas as pd
true = pd.read_csv('/content/True.csv')
fake = pd.read_csv('/content/Fake.csv')

# add a 'label' column to distinguish between true and fake news
true['label'] = 'true'
fake['label'] = 'fake'
print('add label success')

combined_df = pd.concat([true, fake], ignore_index=True)

combined_df.drop_duplicates(inplace=True)
print("Number of duplicate rows:", combined_df.duplicated().sum())

print("Number of missing rows:\n", combined_df.isnull().sum())

display(combined_df.head())
display(combined_df.tail())
print('combine dataset success')

"""cleaning text for analysis"""

import string
import nltk
from nltk.corpus import stopwords

# convert lowercase
combined_df['text'] = combined_df['text'].str.lower()
combined_df['title'] = combined_df['title'].str.lower()
print('convert to lowercase success')

# remove any punctuation
combined_df['text'] = combined_df['text'].str.translate(str.maketrans('', '', string.punctuation))
combined_df['title'] = combined_df['title'].str.translate(str.maketrans('', '', string.punctuation))
print('remove punctuation success')

# download stopwords
try:
    stopwords = set(stopwords.words('english'))
except LookupError:
    nltk.download('stopwords')
    stopwords = set(stopwords.words('english'))

# remove stop words
def remove_stopwords(text):
    return ' '.join([word for word in str(text).split() if word not in stopwords])

combined_df['text'] = combined_df['text'].apply(lambda x: remove_stopwords(x))
combined_df['title'] = combined_df['title'].apply(lambda x: remove_stopwords(x))

display(combined_df.head())

import nltk
from nltk.tokenize import word_tokenize

# download punkt and punkt tab for tokenization
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

try:
    nltk.data.find('tokenizers/punkt_tab')
except LookupError:
    nltk.download('punkt_tab')

# tokenize text
def tokenize_text(text):
    return word_tokenize(text)

combined_df['tokens_text'] = combined_df['text'].apply(tokenize_text)
combined_df['tokens_title'] = combined_df['title'].apply(tokenize_text)

display(combined_df.head())

clean_df = combined_df.drop(columns=['text', 'title', 'subject', 'date'])
display(clean_df.head())

"""# classification

"""

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf_vectorizer = TfidfVectorizer()

# Join tokens back into strings for the vectorizer
clean_df['text_string'] = clean_df['tokens_text'].apply(lambda x: ' '.join(x))
clean_df['title_string'] = clean_df['tokens_title'].apply(lambda x: ' '.join(x))

# Combine text and title strings
clean_df['combined_text'] = clean_df['text_string'] + ' ' + clean_df['title_string']

tfidf_features = tfidf_vectorizer.fit_transform(clean_df['combined_text'])

print("TF-IDF features shape:", tfidf_features.shape)

"""Model selection

"""

from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from sklearn.ensemble import RandomForestClassifier

models = {
    'Multinomial Naive Bayes': MultinomialNB(),
    'Logistic Regression': LogisticRegression(max_iter=1000),
    'Linear SVC': LinearSVC(),
    'Random Forest': RandomForestClassifier()
}

print("Selected models:", list(models.keys()))

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(tfidf_features, clean_df['label'], test_size=0.2, random_state=42)

trained_models = {}
for name, model in models.items():
    print(f"Training {name}...")
    model.fit(X_train, y_train)
    trained_models[name] = model
    print(f"{name} training complete.")

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

evaluation_results = {}

for name, model in trained_models.items():
    print(f"Evaluating {name}...")
    y_pred = model.predict(X_test)

    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, pos_label='true')
    recall = recall_score(y_test, y_pred, pos_label='true')
    f1 = f1_score(y_test, y_pred, pos_label='true')

    evaluation_results[name] = {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1_score': f1
    }
    print(f"{name} evaluation complete.")

print("\nEvaluation Results:")
print(evaluation_results)

print("Evaluation Results:")
for name, metrics in evaluation_results.items():
    print(f"\n{name}:")
    for metric, value in metrics.items():
        print(f"  {metric}: {value:.4f}")

# Identify the best model based on F1-score
best_model_name = max(evaluation_results, key=lambda k: evaluation_results[k]['f1_score'])
best_model_metrics = evaluation_results[best_model_name]

print(f"\nBest performing model based on F1-score: {best_model_name}")
print(f"Metrics for the best model:")
for metric, value in best_model_metrics.items():
    print(f"  {metric}: {value:.4f}")

"""tune-in hyperparameter best performing model"""

param_grid = {
    'C': [0.1, 1, 10, 100],
    'loss': ['hinge', 'squared_hinge'],
    'max_iter': [1000, 5000, 10000] # Added max_iter to the parameter grid
}

print(param_grid)

from sklearn.model_selection import GridSearchCV

linear_svc = LinearSVC(random_state=42)

grid_search = GridSearchCV(linear_svc, param_grid, cv=5)

grid_search.fit(X_train, y_train)

print("Best hyperparameters found:")
print(grid_search.best_params_)

best_linear_svc = grid_search.best_estimator_

y_pred_best_svc = best_linear_svc.predict(X_test)

best_svc_accuracy = accuracy_score(y_test, y_pred_best_svc)
best_svc_precision = precision_score(y_test, y_pred_best_svc, pos_label='true')
best_svc_recall = recall_score(y_test, y_pred_best_svc, pos_label='true')
best_svc_f1 = f1_score(y_test, y_pred_best_svc, pos_label='true')

best_svc_evaluation_results = {
    'accuracy': best_svc_accuracy,
    'precision': best_svc_precision,
    'recall': best_svc_recall,
    'f1_score': best_svc_f1
}

print("Evaluation Results for Best Linear SVC Model:")
for metric, value in best_svc_evaluation_results.items():
    print(f"  {metric}: {value:.4f}")

print("Evaluation Results of Previously Trained Models:")
for name, metrics in evaluation_results.items():
    print(f"\n{name}:")
    for metric, value in metrics.items():
        print(f"  {metric}: {value:.4f}")

print("\nEvaluation Results for Tuned Linear SVC Model:")
for metric, value in best_svc_evaluation_results.items():
    print(f"  {metric}: {value:.4f}")

# Get the coefficients from the best Linear SVC model
coefficients = best_linear_svc.coef_[0]

# Get the feature names from the TF-IDF vectorizer
feature_names = tfidf_vectorizer.get_feature_names_out()

# Create a pandas Series of coefficients with feature names as index
coef_series = pd.Series(coefficients, index=feature_names)

# Sort the coefficients by their absolute values to get the most important features
top_features = coef_series.abs().sort_values(ascending=False)

# Display the top N features
top_n = 20
print(f"Top {top_n} most important features (based on absolute coefficient values):")
display(top_features.head(top_n))

print("\nTop features with their coefficient values:")
display(coef_series.loc[top_features.head(top_n).index])

"""# visualzation"""

import matplotlib.pyplot as plt
import seaborn as sns

top_features_viz = top_features.head(top_n)

plt.figure(figsize=(12, 8))
sns.barplot(x=top_features_viz.index, y=top_features_viz.values, palette='viridis')
plt.xticks(rotation=35)
plt.title(f'Top {top_n} Most Important Features (Absolute Coefficient Values)')
plt.xlabel('Features')
plt.ylabel('Absolute Coefficient Value')
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

top_features_coef_viz = coef_series.loc[top_features.head(top_n).index]

plt.figure(figsize=(16, 8))
sns.barplot(x=top_features_coef_viz.index, y=top_features_coef_viz.values, palette='coolwarm', width=0.8)
plt.xticks(rotation=45, ha='right')
plt.title(f'Top {top_n} Most Important Features (Coefficient Values)')
plt.xlabel('Features')
plt.ylabel('Coefficient Value')
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.axhline(0, color='black', linewidth=2)
plt.tight_layout()
plt.show()

"""Based on this analysis, the results can be used to advise the public on sorting circulating news. If the news contains words with positive coefficients, it is likely to be authentic news. Conversely, if the news contains words with negative coefficients, it should be suspected as fake news."""